{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sec-edgar-downloader\n",
      "  Downloading sec_edgar_downloader-5.0.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from sec-edgar-downloader) (2.31.0)\n",
      "Collecting pyrate-limiter>=3.1.0 (from sec-edgar-downloader)\n",
      "  Downloading pyrate_limiter-3.6.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from requests->sec-edgar-downloader) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from requests->sec-edgar-downloader) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from requests->sec-edgar-downloader) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from requests->sec-edgar-downloader) (2023.7.22)\n",
      "Downloading sec_edgar_downloader-5.0.2-py3-none-any.whl (14 kB)\n",
      "Downloading pyrate_limiter-3.6.1-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: pyrate-limiter, sec-edgar-downloader\n",
      "Successfully installed pyrate-limiter-3.6.1 sec-edgar-downloader-5.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sec-edgar-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sec_edgar_downloader import Downloader\n",
    "dl = Downloader(\"MyCompanyName\", \"my.email@domain.com\")\n",
    "dl.get(\"10-K\", \"MSFT\", after=\"2017-01-01\", before=\"2019-03-25\", download_details=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All text files have been merged and cleaned. Merged content written to merged_files_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the directory path\n",
    "directory = r'C:\\Users\\harsh\\OneDrive\\Desktop\\GeorgiaTech\\sec-edgar-filings\\MSFT'\n",
    "\n",
    "# Create a list to store all text files\n",
    "text_files = []\n",
    "\n",
    "# Recursive function to traverse directories and collect text files\n",
    "def collect_text_files(directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                text_files.append(file_path)\n",
    "\n",
    "# Call the function to collect text files\n",
    "collect_text_files(directory)\n",
    "\n",
    "# Create a list to store cleaned text content\n",
    "cleaned_contents = []\n",
    "\n",
    "# Iterate over each text file, remove HTML tags, and add cleaned content to the list\n",
    "for file_path in text_files:\n",
    "    with open(file_path, 'r') as file:\n",
    "        html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        text_content = soup.get_text()\n",
    "        cleaned_contents.append(text_content)\n",
    "\n",
    "# Write the merged contents to a single text file with UTF-8 encoding\n",
    "output_file = 'merged_files_cleaned.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for contents in cleaned_contents:\n",
    "        file.write(contents)\n",
    "        file.write('\\n\\n')\n",
    "\n",
    "print(f'All text files have been merged and cleaned. Merged content written to {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All textual content from 10-K files have been merged and written to merged_files_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the directory path\n",
    "directory = r'C:\\Users\\harsh\\OneDrive\\Desktop\\GeorgiaTech\\sec-edgar-filings\\MSFT'\n",
    "\n",
    "# Regular expressions for pattern matching\n",
    "text_start_pattern = re.compile(r'<DOCUMENT>') \n",
    "text_end_pattern = re.compile(r'</DOCUMENT>')\n",
    "type_pattern = re.compile(r'<TYPE>10-K[^\\n]+')\n",
    "\n",
    "# Function to extract textual content from 10-K files\n",
    "def textual_content(file_content):\n",
    "    doc_start_list = [x.start() for x in text_start_pattern.finditer(file_content)]\n",
    "    doc_end_list = [x.end() for x in text_end_pattern.finditer(file_content)]\n",
    "    type_list = [x[len('<TYPE>'):] for x in type_pattern.findall(file_content)]\n",
    "    \n",
    "    textual_data = []\n",
    "    for doc_type, start_index, end_index in zip(type_list, doc_start_list, doc_end_list):\n",
    "        report_content = file_content[start_index:end_index]\n",
    "        textual_data.append(report_content)\n",
    "    \n",
    "    return textual_data\n",
    "\n",
    "# Recursive function to traverse directories and collect text files\n",
    "def collect_text_files(directory):\n",
    "    text_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    file_content = file.read()\n",
    "                    text_files.extend(textual_content(file_content))\n",
    "    return text_files\n",
    "\n",
    "# Call the function to collect text files\n",
    "text_files = collect_text_files(directory)\n",
    "\n",
    "# Write the merged contents to a single text file with UTF-8 encoding\n",
    "output_file = 'merged_files_cleaned.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for contents in text_files:\n",
    "        file.write(contents)\n",
    "        file.write('\\n\\n')\n",
    "\n",
    "print(f'All textual content from 10-K files have been merged and written to {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All text files have been merged and cleaned. Merged content written to merged_files_cleaned_new.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "# Define the directory path\n",
    "directory = r'C:\\Users\\harsh\\OneDrive\\Desktop\\GeorgiaTech\\sec-edgar-filings\\MSFT'\n",
    "\n",
    "# Create a list to store all text files\n",
    "text_files = []\n",
    "\n",
    "# Recursive function to traverse directories and collect text files\n",
    "def collect_text_files(directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                text_files.append(file_path)\n",
    "\n",
    "# Call the function to collect text files\n",
    "collect_text_files(directory)\n",
    "\n",
    "# Create a list to store cleaned text content\n",
    "cleaned_contents = []\n",
    "\n",
    "# Iterate over each text file, remove HTML tags, non-printable characters, and add cleaned content to the list\n",
    "for file_path in text_files:\n",
    "    with open(file_path, 'r') as file:\n",
    "        html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        text_content = soup.get_text()\n",
    "        cleaned_text = ''.join(ch for ch in text_content if unicodedata.category(ch)[0] != 'C')  # Remove non-printable characters\n",
    "        cleaned_contents.append(cleaned_text)\n",
    "\n",
    "# Write the merged contents to a single text file with UTF-8 encoding\n",
    "output_file = 'merged_files_cleaned_new.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for contents in cleaned_contents:\n",
    "        file.write(contents)\n",
    "        file.write('\\n\\n')\n",
    "\n",
    "print(f'All text files have been merged and cleaned. Merged content written to {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new just two \n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the directory path\n",
    "directory = r'C:\\Users\\harsh\\OneDrive\\Desktop\\GeorgiaTech\\sec-edgar-filings\\MSFT'\n",
    "\n",
    "# Create a dictionary to store file contents by year\n",
    "file_contents = {}\n",
    "\n",
    "# Recursive function to traverse directories and collect text files\n",
    "def collect_text_files(directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.txt'):\n",
    "                # Extract the year from the filename\n",
    "                parts = filename.split('-')\n",
    "                year = parts[-2]\n",
    "                \n",
    "                # Construct the full path to the file\n",
    "                file_path = os.path.join(root, filename)\n",
    "                \n",
    "                # Read the contents of the file\n",
    "                with open(file_path, 'r') as file:\n",
    "                    html_content = file.read()\n",
    "                \n",
    "                # Remove HTML tags and clean the text\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                text_content = soup.get_text()\n",
    "                \n",
    "                # Add the cleaned text content to the dictionary under the corresponding year\n",
    "                if year in file_contents:\n",
    "                    file_contents[year].append(text_content)\n",
    "                else:\n",
    "                    file_contents[year] = [text_content]\n",
    "\n",
    "# Call the function to collect text files\n",
    "collect_text_files(directory)\n",
    "\n",
    "# Write the grouped contents to a single text file\n",
    "output_file = 'grouped_files_cleaned.txt'\n",
    "with open(output_file, 'w') as file:\n",
    "    for year, contents_list in file_contents.items():\n",
    "        file.write(f'Year: {year}\\n\\n')\n",
    "        for contents in contents_list:\n",
    "            file.write(contents)\n",
    "            file.write('\\n\\n')\n",
    "\n",
    "print(f'Grouped and cleaned files written to {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped files written to grouped_files.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "directory = r'C:\\Users\\harsh\\OneDrive\\Desktop\\GeorgiaTech\\sec-edgar-filings\\MSFT'\n",
    "\n",
    "# Create a dictionary to store file contents by year\n",
    "file_contents = {}\n",
    "\n",
    "# Recursive function to traverse directories and collect text files\n",
    "def collect_text_files(directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.txt'):\n",
    "                # Extract the year from the filename\n",
    "                parts = filename.split('-')\n",
    "                year = parts[-2]\n",
    "                \n",
    "                # Construct the full path to the file\n",
    "                file_path = os.path.join(root, filename)\n",
    "                \n",
    "                # Read the contents of the file\n",
    "                with open(file_path, 'r') as file:\n",
    "                    contents = file.read()\n",
    "                \n",
    "                # Add the contents to the dictionary under the corresponding year\n",
    "                if year in file_contents:\n",
    "                    file_contents[year].append(contents)\n",
    "                else:\n",
    "                    file_contents[year] = [contents]\n",
    "\n",
    "# Call the function to collect text files\n",
    "collect_text_files(directory)\n",
    "\n",
    "# Write the grouped contents to a single text file\n",
    "output_file = 'grouped_files.txt'\n",
    "with open(output_file, 'w') as file:\n",
    "    for year, contents_list in file_contents.items():\n",
    "        file.write(f'Year: {year}\\n\\n')\n",
    "        for contents in contents_list:\n",
    "            file.write(contents)\n",
    "            file.write('\\n\\n')\n",
    "\n",
    "print(f'Grouped files written to {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags_from_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        processed_text = remove_html_tags(text)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as output:\n",
    "        output.write(processed_text)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
